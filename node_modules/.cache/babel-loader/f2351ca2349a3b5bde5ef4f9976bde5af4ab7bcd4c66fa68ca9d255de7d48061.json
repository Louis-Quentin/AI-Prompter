{"ast":null,"code":"// src/chatgpt-api.ts\nimport Keyv from \"keyv\";\nimport pTimeout from \"p-timeout\";\nimport QuickLRU from \"quick-lru\";\nimport { v4 as uuidv4 } from \"uuid\";\n\n// src/tokenizer.ts\nimport { get_encoding } from \"@dqbd/tiktoken\";\nvar tokenizer = get_encoding(\"cl100k_base\");\nfunction encode(input) {\n  return tokenizer.encode(input);\n}\n\n// src/types.ts\nvar ChatGPTError = class extends Error {};\nvar openai;\n(openai2 => {})(openai || (openai = {}));\n\n// src/fetch.ts\nvar fetch = globalThis.fetch;\n\n// src/fetch-sse.ts\nimport { createParser } from \"eventsource-parser\";\n\n// src/stream-async-iterable.ts\nasync function* streamAsyncIterable(stream) {\n  const reader = stream.getReader();\n  try {\n    while (true) {\n      const {\n        done,\n        value\n      } = await reader.read();\n      if (done) {\n        return;\n      }\n      yield value;\n    }\n  } finally {\n    reader.releaseLock();\n  }\n}\n\n// src/fetch-sse.ts\nasync function fetchSSE(url, options) {\n  let fetch2 = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : fetch;\n  const {\n    onMessage,\n    onError,\n    ...fetchOptions\n  } = options;\n  const res = await fetch2(url, fetchOptions);\n  if (!res.ok) {\n    let reason;\n    try {\n      reason = await res.text();\n    } catch (err) {\n      reason = res.statusText;\n    }\n    const msg = `ChatGPT error ${res.status}: ${reason}`;\n    const error = new ChatGPTError(msg, {\n      cause: res\n    });\n    error.statusCode = res.status;\n    error.statusText = res.statusText;\n    throw error;\n  }\n  const parser = createParser(event => {\n    if (event.type === \"event\") {\n      onMessage(event.data);\n    }\n  });\n  const feed = chunk => {\n    var _a;\n    let response = null;\n    try {\n      response = JSON.parse(chunk);\n    } catch {}\n    if (((_a = response == null ? void 0 : response.detail) == null ? void 0 : _a.type) === \"invalid_request_error\") {\n      const msg = `ChatGPT error ${response.detail.message}: ${response.detail.code} (${response.detail.type})`;\n      const error = new ChatGPTError(msg, {\n        cause: response\n      });\n      error.statusCode = response.detail.code;\n      error.statusText = response.detail.message;\n      if (onError) {\n        onError(error);\n      } else {\n        console.error(error);\n      }\n      return;\n    }\n    parser.feed(chunk);\n  };\n  if (!res.body.getReader) {\n    const body = res.body;\n    if (!body.on || !body.read) {\n      throw new ChatGPTError('unsupported \"fetch\" implementation');\n    }\n    body.on(\"readable\", () => {\n      let chunk;\n      while (null !== (chunk = body.read())) {\n        feed(chunk.toString());\n      }\n    });\n  } else {\n    for await (const chunk of streamAsyncIterable(res.body)) {\n      const str = new TextDecoder().decode(chunk);\n      feed(str);\n    }\n  }\n}\n\n// src/chatgpt-api.ts\nvar CHATGPT_MODEL = \"gpt-3.5-turbo\";\nvar USER_LABEL_DEFAULT = \"User\";\nvar ASSISTANT_LABEL_DEFAULT = \"ChatGPT\";\nvar ChatGPTAPI = class {\n  /**\n   * Creates a new client wrapper around OpenAI's chat completion API, mimicing the official ChatGPT webapp's functionality as closely as possible.\n   *\n   * @param apiKey - OpenAI API key (required).\n   * @param apiOrg - Optional OpenAI API organization (optional).\n   * @param apiBaseUrl - Optional override for the OpenAI API base URL.\n   * @param debug - Optional enables logging debugging info to stdout.\n   * @param completionParams - Param overrides to send to the [OpenAI chat completion API](https://platform.openai.com/docs/api-reference/chat/create). Options like `temperature` and `presence_penalty` can be tweaked to change the personality of the assistant.\n   * @param maxModelTokens - Optional override for the maximum number of tokens allowed by the model's context. Defaults to 4096.\n   * @param maxResponseTokens - Optional override for the minimum number of tokens allowed for the model's response. Defaults to 1000.\n   * @param messageStore - Optional [Keyv](https://github.com/jaredwray/keyv) store to persist chat messages to. If not provided, messages will be lost when the process exits.\n   * @param getMessageById - Optional function to retrieve a message by its ID. If not provided, the default implementation will be used (using an in-memory `messageStore`).\n   * @param upsertMessage - Optional function to insert or update a message. If not provided, the default implementation will be used (using an in-memory `messageStore`).\n   * @param fetch - Optional override for the `fetch` implementation to use. Defaults to the global `fetch` function.\n   */\n  constructor(opts) {\n    const {\n      apiKey,\n      apiOrg,\n      apiBaseUrl = \"https://api.openai.com/v1\",\n      debug = false,\n      messageStore,\n      completionParams,\n      systemMessage,\n      maxModelTokens = 4e3,\n      maxResponseTokens = 1e3,\n      getMessageById,\n      upsertMessage,\n      fetch: fetch2 = fetch\n    } = opts;\n    this._apiKey = apiKey;\n    this._apiOrg = apiOrg;\n    this._apiBaseUrl = apiBaseUrl;\n    this._debug = !!debug;\n    this._fetch = fetch2;\n    this._completionParams = {\n      model: CHATGPT_MODEL,\n      temperature: 0.8,\n      top_p: 1,\n      presence_penalty: 1,\n      ...completionParams\n    };\n    this._systemMessage = systemMessage;\n    if (this._systemMessage === void 0) {\n      const currentDate = /* @__PURE__ */new Date().toISOString().split(\"T\")[0];\n      this._systemMessage = `You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible.\nKnowledge cutoff: 2021-09-01\nCurrent date: ${currentDate}`;\n    }\n    this._maxModelTokens = maxModelTokens;\n    this._maxResponseTokens = maxResponseTokens;\n    this._getMessageById = getMessageById ?? this._defaultGetMessageById;\n    this._upsertMessage = upsertMessage ?? this._defaultUpsertMessage;\n    if (messageStore) {\n      this._messageStore = messageStore;\n    } else {\n      this._messageStore = new Keyv({\n        store: new QuickLRU({\n          maxSize: 1e4\n        })\n      });\n    }\n    if (!this._apiKey) {\n      throw new Error(\"OpenAI missing required apiKey\");\n    }\n    if (!this._fetch) {\n      throw new Error(\"Invalid environment; fetch is not defined\");\n    }\n    if (typeof this._fetch !== \"function\") {\n      throw new Error('Invalid \"fetch\" is not a function');\n    }\n  }\n  /**\n   * Sends a message to the OpenAI chat completions endpoint, waits for the response\n   * to resolve, and returns the response.\n   *\n   * If you want your response to have historical context, you must provide a valid `parentMessageId`.\n   *\n   * If you want to receive a stream of partial responses, use `opts.onProgress`.\n   *\n   * Set `debug: true` in the `ChatGPTAPI` constructor to log more info on the full prompt sent to the OpenAI chat completions API. You can override the `systemMessage` in `opts` to customize the assistant's instructions.\n   *\n   * @param message - The prompt message to send\n   * @param opts.parentMessageId - Optional ID of the previous message in the conversation (defaults to `undefined`)\n   * @param opts.conversationId - Optional ID of the conversation (defaults to `undefined`)\n   * @param opts.messageId - Optional ID of the message to send (defaults to a random UUID)\n   * @param opts.systemMessage - Optional override for the chat \"system message\" which acts as instructions to the model (defaults to the ChatGPT system message)\n   * @param opts.timeoutMs - Optional timeout in milliseconds (defaults to no timeout)\n   * @param opts.onProgress - Optional callback which will be invoked every time the partial response is updated\n   * @param opts.abortSignal - Optional callback used to abort the underlying `fetch` call using an [AbortController](https://developer.mozilla.org/en-US/docs/Web/API/AbortController)\n   * @param completionParams - Optional overrides to send to the [OpenAI chat completion API](https://platform.openai.com/docs/api-reference/chat/create). Options like `temperature` and `presence_penalty` can be tweaked to change the personality of the assistant.\n   *\n   * @returns The response from ChatGPT\n   */\n  async sendMessage(text) {\n    let opts = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    const {\n      parentMessageId,\n      messageId = uuidv4(),\n      timeoutMs,\n      onProgress,\n      stream = onProgress ? true : false,\n      completionParams,\n      conversationId\n    } = opts;\n    let {\n      abortSignal\n    } = opts;\n    let abortController = null;\n    if (timeoutMs && !abortSignal) {\n      abortController = new AbortController();\n      abortSignal = abortController.signal;\n    }\n    const message = {\n      role: \"user\",\n      id: messageId,\n      conversationId,\n      parentMessageId,\n      text\n    };\n    const latestQuestion = message;\n    const {\n      messages,\n      maxTokens,\n      numTokens\n    } = await this._buildMessages(text, opts);\n    const result = {\n      role: \"assistant\",\n      id: uuidv4(),\n      conversationId,\n      parentMessageId: messageId,\n      text: \"\"\n    };\n    const responseP = new Promise(async (resolve, reject) => {\n      var _a, _b;\n      const url = `${this._apiBaseUrl}/chat/completions`;\n      const headers = {\n        \"Content-Type\": \"application/json\",\n        Authorization: `Bearer ${this._apiKey}`\n      };\n      const body = {\n        max_tokens: maxTokens,\n        ...this._completionParams,\n        ...completionParams,\n        messages,\n        stream\n      };\n      if (this._apiOrg) {\n        headers[\"OpenAI-Organization\"] = this._apiOrg;\n      }\n      if (this._debug) {\n        console.log(`sendMessage (${numTokens} tokens)`, body);\n      }\n      if (stream) {\n        fetchSSE(url, {\n          method: \"POST\",\n          headers,\n          body: JSON.stringify(body),\n          signal: abortSignal,\n          onMessage: data => {\n            var _a2;\n            if (data === \"[DONE]\") {\n              result.text = result.text.trim();\n              return resolve(result);\n            }\n            try {\n              const response = JSON.parse(data);\n              if (response.id) {\n                result.id = response.id;\n              }\n              if ((_a2 = response.choices) == null ? void 0 : _a2.length) {\n                const delta = response.choices[0].delta;\n                result.delta = delta.content;\n                if (delta == null ? void 0 : delta.content) result.text += delta.content;\n                if (delta.role) {\n                  result.role = delta.role;\n                }\n                result.detail = response;\n                onProgress == null ? void 0 : onProgress(result);\n              }\n            } catch (err) {\n              console.warn(\"OpenAI stream SEE event unexpected error\", err);\n              return reject(err);\n            }\n          }\n        }, this._fetch).catch(reject);\n      } else {\n        try {\n          const res = await this._fetch(url, {\n            method: \"POST\",\n            headers,\n            body: JSON.stringify(body),\n            signal: abortSignal\n          });\n          if (!res.ok) {\n            const reason = await res.text();\n            const msg = `OpenAI error ${res.status || res.statusText}: ${reason}`;\n            const error = new ChatGPTError(msg, {\n              cause: res\n            });\n            error.statusCode = res.status;\n            error.statusText = res.statusText;\n            return reject(error);\n          }\n          const response = await res.json();\n          if (this._debug) {\n            console.log(response);\n          }\n          if (response == null ? void 0 : response.id) {\n            result.id = response.id;\n          }\n          if ((_a = response == null ? void 0 : response.choices) == null ? void 0 : _a.length) {\n            const message2 = response.choices[0].message;\n            result.text = message2.content;\n            if (message2.role) {\n              result.role = message2.role;\n            }\n          } else {\n            const res2 = response;\n            return reject(new Error(`OpenAI error: ${((_b = res2 == null ? void 0 : res2.detail) == null ? void 0 : _b.message) || (res2 == null ? void 0 : res2.detail) || \"unknown\"}`));\n          }\n          result.detail = response;\n          return resolve(result);\n        } catch (err) {\n          return reject(err);\n        }\n      }\n    }).then(async message2 => {\n      if (message2.detail && !message2.detail.usage) {\n        try {\n          const promptTokens = numTokens;\n          const completionTokens = await this._getTokenCount(message2.text);\n          message2.detail.usage = {\n            prompt_tokens: promptTokens,\n            completion_tokens: completionTokens,\n            total_tokens: promptTokens + completionTokens,\n            estimated: true\n          };\n        } catch (err) {}\n      }\n      return Promise.all([this._upsertMessage(latestQuestion), this._upsertMessage(message2)]).then(() => message2);\n    });\n    if (timeoutMs) {\n      if (abortController) {\n        ;\n        responseP.cancel = () => {\n          abortController.abort();\n        };\n      }\n      return pTimeout(responseP, {\n        milliseconds: timeoutMs,\n        message: \"OpenAI timed out waiting for response\"\n      });\n    } else {\n      return responseP;\n    }\n  }\n  get apiKey() {\n    return this._apiKey;\n  }\n  set apiKey(apiKey) {\n    this._apiKey = apiKey;\n  }\n  get apiOrg() {\n    return this._apiOrg;\n  }\n  set apiOrg(apiOrg) {\n    this._apiOrg = apiOrg;\n  }\n  async _buildMessages(text, opts) {\n    const {\n      systemMessage = this._systemMessage\n    } = opts;\n    let {\n      parentMessageId\n    } = opts;\n    const userLabel = USER_LABEL_DEFAULT;\n    const assistantLabel = ASSISTANT_LABEL_DEFAULT;\n    const maxNumTokens = this._maxModelTokens - this._maxResponseTokens;\n    let messages = [];\n    if (systemMessage) {\n      messages.push({\n        role: \"system\",\n        content: systemMessage\n      });\n    }\n    const systemMessageOffset = messages.length;\n    let nextMessages = text ? messages.concat([{\n      role: \"user\",\n      content: text,\n      name: opts.name\n    }]) : messages;\n    let numTokens = 0;\n    do {\n      const prompt = nextMessages.reduce((prompt2, message) => {\n        switch (message.role) {\n          case \"system\":\n            return prompt2.concat([`Instructions:\n${message.content}`]);\n          case \"user\":\n            return prompt2.concat([`${userLabel}:\n${message.content}`]);\n          default:\n            return prompt2.concat([`${assistantLabel}:\n${message.content}`]);\n        }\n      }, []).join(\"\\n\\n\");\n      const nextNumTokensEstimate = await this._getTokenCount(prompt);\n      const isValidPrompt = nextNumTokensEstimate <= maxNumTokens;\n      if (prompt && !isValidPrompt) {\n        break;\n      }\n      messages = nextMessages;\n      numTokens = nextNumTokensEstimate;\n      if (!isValidPrompt) {\n        break;\n      }\n      if (!parentMessageId) {\n        break;\n      }\n      const parentMessage = await this._getMessageById(parentMessageId);\n      if (!parentMessage) {\n        break;\n      }\n      const parentMessageRole = parentMessage.role || \"user\";\n      nextMessages = nextMessages.slice(0, systemMessageOffset).concat([{\n        role: parentMessageRole,\n        content: parentMessage.text,\n        name: parentMessage.name\n      }, ...nextMessages.slice(systemMessageOffset)]);\n      parentMessageId = parentMessage.parentMessageId;\n    } while (true);\n    const maxTokens = Math.max(1, Math.min(this._maxModelTokens - numTokens, this._maxResponseTokens));\n    return {\n      messages,\n      maxTokens,\n      numTokens\n    };\n  }\n  async _getTokenCount(text) {\n    text = text.replace(/<\\|endoftext\\|>/g, \"\");\n    return encode(text).length;\n  }\n  async _defaultGetMessageById(id) {\n    const res = await this._messageStore.get(id);\n    return res;\n  }\n  async _defaultUpsertMessage(message) {\n    await this._messageStore.set(message.id, message);\n  }\n};\n\n// src/chatgpt-unofficial-proxy-api.ts\nimport pTimeout2 from \"p-timeout\";\nimport { v4 as uuidv42 } from \"uuid\";\n\n// src/utils.ts\nvar uuidv4Re = /^[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}$/i;\nfunction isValidUUIDv4(str) {\n  return str && uuidv4Re.test(str);\n}\n\n// src/chatgpt-unofficial-proxy-api.ts\nvar ChatGPTUnofficialProxyAPI = class {\n  /**\n   * @param fetch - Optional override for the `fetch` implementation to use. Defaults to the global `fetch` function.\n   */\n  constructor(opts) {\n    const {\n      accessToken,\n      apiReverseProxyUrl = \"https://bypass.duti.tech/api/conversation\",\n      model = \"text-davinci-002-render-sha\",\n      debug = false,\n      headers,\n      fetch: fetch2 = fetch\n    } = opts;\n    this._accessToken = accessToken;\n    this._apiReverseProxyUrl = apiReverseProxyUrl;\n    this._debug = !!debug;\n    this._model = model;\n    this._fetch = fetch2;\n    this._headers = headers;\n    if (!this._accessToken) {\n      throw new Error(\"ChatGPT invalid accessToken\");\n    }\n    if (!this._fetch) {\n      throw new Error(\"Invalid environment; fetch is not defined\");\n    }\n    if (typeof this._fetch !== \"function\") {\n      throw new Error('Invalid \"fetch\" is not a function');\n    }\n  }\n  get accessToken() {\n    return this._accessToken;\n  }\n  set accessToken(value) {\n    this._accessToken = value;\n  }\n  /**\n   * Sends a message to ChatGPT, waits for the response to resolve, and returns\n   * the response.\n   *\n   * If you want your response to have historical context, you must provide a valid `parentMessageId`.\n   *\n   * If you want to receive a stream of partial responses, use `opts.onProgress`.\n   * If you want to receive the full response, including message and conversation IDs,\n   * you can use `opts.onConversationResponse` or use the `ChatGPTAPI.getConversation`\n   * helper.\n   *\n   * Set `debug: true` in the `ChatGPTAPI` constructor to log more info on the full prompt sent to the OpenAI completions API. You can override the `promptPrefix` and `promptSuffix` in `opts` to customize the prompt.\n   *\n   * @param message - The prompt message to send\n   * @param opts.conversationId - Optional ID of a conversation to continue (defaults to a random UUID)\n   * @param opts.parentMessageId - Optional ID of the previous message in the conversation (defaults to `undefined`)\n   * @param opts.messageId - Optional ID of the message to send (defaults to a random UUID)\n   * @param opts.timeoutMs - Optional timeout in milliseconds (defaults to no timeout)\n   * @param opts.onProgress - Optional callback which will be invoked every time the partial response is updated\n   * @param opts.abortSignal - Optional callback used to abort the underlying `fetch` call using an [AbortController](https://developer.mozilla.org/en-US/docs/Web/API/AbortController)\n   *\n   * @returns The response from ChatGPT\n   */\n  async sendMessage(text) {\n    let opts = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    if (!!opts.conversationId !== !!opts.parentMessageId) {\n      throw new Error(\"ChatGPTUnofficialProxyAPI.sendMessage: conversationId and parentMessageId must both be set or both be undefined\");\n    }\n    if (opts.conversationId && !isValidUUIDv4(opts.conversationId)) {\n      throw new Error(\"ChatGPTUnofficialProxyAPI.sendMessage: conversationId is not a valid v4 UUID\");\n    }\n    if (opts.parentMessageId && !isValidUUIDv4(opts.parentMessageId)) {\n      throw new Error(\"ChatGPTUnofficialProxyAPI.sendMessage: parentMessageId is not a valid v4 UUID\");\n    }\n    if (opts.messageId && !isValidUUIDv4(opts.messageId)) {\n      throw new Error(\"ChatGPTUnofficialProxyAPI.sendMessage: messageId is not a valid v4 UUID\");\n    }\n    const {\n      conversationId,\n      parentMessageId = uuidv42(),\n      messageId = uuidv42(),\n      action = \"next\",\n      timeoutMs,\n      onProgress\n    } = opts;\n    let {\n      abortSignal\n    } = opts;\n    let abortController = null;\n    if (timeoutMs && !abortSignal) {\n      abortController = new AbortController();\n      abortSignal = abortController.signal;\n    }\n    const body = {\n      action,\n      messages: [{\n        id: messageId,\n        role: \"user\",\n        content: {\n          content_type: \"text\",\n          parts: [text]\n        }\n      }],\n      model: this._model,\n      parent_message_id: parentMessageId\n    };\n    if (conversationId) {\n      body.conversation_id = conversationId;\n    }\n    const result = {\n      role: \"assistant\",\n      id: uuidv42(),\n      parentMessageId: messageId,\n      conversationId,\n      text: \"\"\n    };\n    const responseP = new Promise((resolve, reject) => {\n      const url = this._apiReverseProxyUrl;\n      const headers = {\n        ...this._headers,\n        Authorization: `Bearer ${this._accessToken}`,\n        Accept: \"text/event-stream\",\n        \"Content-Type\": \"application/json\"\n      };\n      if (this._debug) {\n        console.log(\"POST\", url, {\n          body,\n          headers\n        });\n      }\n      fetchSSE(url, {\n        method: \"POST\",\n        headers,\n        body: JSON.stringify(body),\n        signal: abortSignal,\n        onMessage: data => {\n          var _a, _b, _c;\n          if (data === \"[DONE]\") {\n            return resolve(result);\n          }\n          try {\n            const convoResponseEvent = JSON.parse(data);\n            if (convoResponseEvent.conversation_id) {\n              result.conversationId = convoResponseEvent.conversation_id;\n            }\n            if ((_a = convoResponseEvent.message) == null ? void 0 : _a.id) {\n              result.id = convoResponseEvent.message.id;\n            }\n            const message = convoResponseEvent.message;\n            if (message) {\n              let text2 = (_c = (_b = message == null ? void 0 : message.content) == null ? void 0 : _b.parts) == null ? void 0 : _c[0];\n              if (text2) {\n                result.text = text2;\n                if (onProgress) {\n                  onProgress(result);\n                }\n              }\n            }\n          } catch (err) {\n            reject(err);\n          }\n        },\n        onError: err => {\n          reject(err);\n        }\n      }, this._fetch).catch(err => {\n        const errMessageL = err.toString().toLowerCase();\n        if (result.text && (errMessageL === \"error: typeerror: terminated\" || errMessageL === \"typeerror: terminated\")) {\n          return resolve(result);\n        } else {\n          return reject(err);\n        }\n      });\n    });\n    if (timeoutMs) {\n      if (abortController) {\n        ;\n        responseP.cancel = () => {\n          abortController.abort();\n        };\n      }\n      return pTimeout2(responseP, {\n        milliseconds: timeoutMs,\n        message: \"ChatGPT timed out waiting for response\"\n      });\n    } else {\n      return responseP;\n    }\n  }\n};\nexport { ChatGPTAPI, ChatGPTError, ChatGPTUnofficialProxyAPI, openai };","map":{"version":3,"names":["Keyv","pTimeout","QuickLRU","v4","uuidv4","get_encoding","tokenizer","encode","input","ChatGPTError","Error","openai","openai2","fetch","globalThis","createParser","streamAsyncIterable","stream","reader","getReader","done","value","read","releaseLock","fetchSSE","url","options","fetch2","arguments","length","undefined","onMessage","onError","fetchOptions","res","ok","reason","text","err","statusText","msg","status","error","cause","statusCode","parser","event","type","data","feed","chunk","_a","response","JSON","parse","detail","message","code","console","body","on","toString","str","TextDecoder","decode","CHATGPT_MODEL","USER_LABEL_DEFAULT","ASSISTANT_LABEL_DEFAULT","ChatGPTAPI","constructor","opts","apiKey","apiOrg","apiBaseUrl","debug","messageStore","completionParams","systemMessage","maxModelTokens","maxResponseTokens","getMessageById","upsertMessage","_apiKey","_apiOrg","_apiBaseUrl","_debug","_fetch","_completionParams","model","temperature","top_p","presence_penalty","_systemMessage","currentDate","Date","toISOString","split","_maxModelTokens","_maxResponseTokens","_getMessageById","_defaultGetMessageById","_upsertMessage","_defaultUpsertMessage","_messageStore","store","maxSize","sendMessage","parentMessageId","messageId","timeoutMs","onProgress","conversationId","abortSignal","abortController","AbortController","signal","role","id","latestQuestion","messages","maxTokens","numTokens","_buildMessages","result","responseP","Promise","resolve","reject","_b","headers","Authorization","max_tokens","log","method","stringify","_a2","trim","choices","delta","content","warn","catch","json","message2","res2","then","usage","promptTokens","completionTokens","_getTokenCount","prompt_tokens","completion_tokens","total_tokens","estimated","all","cancel","abort","milliseconds","userLabel","assistantLabel","maxNumTokens","push","systemMessageOffset","nextMessages","concat","name","prompt","reduce","prompt2","join","nextNumTokensEstimate","isValidPrompt","parentMessage","parentMessageRole","slice","Math","max","min","replace","get","set","pTimeout2","uuidv42","uuidv4Re","isValidUUIDv4","test","ChatGPTUnofficialProxyAPI","accessToken","apiReverseProxyUrl","_accessToken","_apiReverseProxyUrl","_model","_headers","action","content_type","parts","parent_message_id","conversation_id","Accept","_c","convoResponseEvent","text2","errMessageL","toLowerCase"],"sources":["/home/lamsellem/r_comps/comps/node_modules/chatgpt/src/chatgpt-api.ts","/home/lamsellem/r_comps/comps/node_modules/chatgpt/src/tokenizer.ts","/home/lamsellem/r_comps/comps/node_modules/chatgpt/src/types.ts","/home/lamsellem/r_comps/comps/node_modules/chatgpt/src/fetch.ts","/home/lamsellem/r_comps/comps/node_modules/chatgpt/src/fetch-sse.ts","/home/lamsellem/r_comps/comps/node_modules/chatgpt/src/stream-async-iterable.ts","/home/lamsellem/r_comps/comps/node_modules/chatgpt/src/chatgpt-unofficial-proxy-api.ts","/home/lamsellem/r_comps/comps/node_modules/chatgpt/src/utils.ts"],"sourcesContent":["import Keyv from 'keyv'\nimport pTimeout from 'p-timeout'\nimport QuickLRU from 'quick-lru'\nimport { v4 as uuidv4 } from 'uuid'\n\nimport * as tokenizer from './tokenizer'\nimport * as types from './types'\nimport { fetch as globalFetch } from './fetch'\nimport { fetchSSE } from './fetch-sse'\n\nconst CHATGPT_MODEL = 'gpt-3.5-turbo'\n\nconst USER_LABEL_DEFAULT = 'User'\nconst ASSISTANT_LABEL_DEFAULT = 'ChatGPT'\n\nexport class ChatGPTAPI {\n  protected _apiKey: string\n  protected _apiBaseUrl: string\n  protected _apiOrg?: string\n  protected _debug: boolean\n\n  protected _systemMessage: string\n  protected _completionParams: Omit<\n    types.openai.CreateChatCompletionRequest,\n    'messages' | 'n'\n  >\n  protected _maxModelTokens: number\n  protected _maxResponseTokens: number\n  protected _fetch: types.FetchFn\n\n  protected _getMessageById: types.GetMessageByIdFunction\n  protected _upsertMessage: types.UpsertMessageFunction\n\n  protected _messageStore: Keyv<types.ChatMessage>\n\n  /**\n   * Creates a new client wrapper around OpenAI's chat completion API, mimicing the official ChatGPT webapp's functionality as closely as possible.\n   *\n   * @param apiKey - OpenAI API key (required).\n   * @param apiOrg - Optional OpenAI API organization (optional).\n   * @param apiBaseUrl - Optional override for the OpenAI API base URL.\n   * @param debug - Optional enables logging debugging info to stdout.\n   * @param completionParams - Param overrides to send to the [OpenAI chat completion API](https://platform.openai.com/docs/api-reference/chat/create). Options like `temperature` and `presence_penalty` can be tweaked to change the personality of the assistant.\n   * @param maxModelTokens - Optional override for the maximum number of tokens allowed by the model's context. Defaults to 4096.\n   * @param maxResponseTokens - Optional override for the minimum number of tokens allowed for the model's response. Defaults to 1000.\n   * @param messageStore - Optional [Keyv](https://github.com/jaredwray/keyv) store to persist chat messages to. If not provided, messages will be lost when the process exits.\n   * @param getMessageById - Optional function to retrieve a message by its ID. If not provided, the default implementation will be used (using an in-memory `messageStore`).\n   * @param upsertMessage - Optional function to insert or update a message. If not provided, the default implementation will be used (using an in-memory `messageStore`).\n   * @param fetch - Optional override for the `fetch` implementation to use. Defaults to the global `fetch` function.\n   */\n  constructor(opts: types.ChatGPTAPIOptions) {\n    const {\n      apiKey,\n      apiOrg,\n      apiBaseUrl = 'https://api.openai.com/v1',\n      debug = false,\n      messageStore,\n      completionParams,\n      systemMessage,\n      maxModelTokens = 4000,\n      maxResponseTokens = 1000,\n      getMessageById,\n      upsertMessage,\n      fetch = globalFetch\n    } = opts\n\n    this._apiKey = apiKey\n    this._apiOrg = apiOrg\n    this._apiBaseUrl = apiBaseUrl\n    this._debug = !!debug\n    this._fetch = fetch\n\n    this._completionParams = {\n      model: CHATGPT_MODEL,\n      temperature: 0.8,\n      top_p: 1.0,\n      presence_penalty: 1.0,\n      ...completionParams\n    }\n\n    this._systemMessage = systemMessage\n\n    if (this._systemMessage === undefined) {\n      const currentDate = new Date().toISOString().split('T')[0]\n      this._systemMessage = `You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible.\\nKnowledge cutoff: 2021-09-01\\nCurrent date: ${currentDate}`\n    }\n\n    this._maxModelTokens = maxModelTokens\n    this._maxResponseTokens = maxResponseTokens\n\n    this._getMessageById = getMessageById ?? this._defaultGetMessageById\n    this._upsertMessage = upsertMessage ?? this._defaultUpsertMessage\n\n    if (messageStore) {\n      this._messageStore = messageStore\n    } else {\n      this._messageStore = new Keyv<types.ChatMessage, any>({\n        store: new QuickLRU<string, types.ChatMessage>({ maxSize: 10000 })\n      })\n    }\n\n    if (!this._apiKey) {\n      throw new Error('OpenAI missing required apiKey')\n    }\n\n    if (!this._fetch) {\n      throw new Error('Invalid environment; fetch is not defined')\n    }\n\n    if (typeof this._fetch !== 'function') {\n      throw new Error('Invalid \"fetch\" is not a function')\n    }\n  }\n\n  /**\n   * Sends a message to the OpenAI chat completions endpoint, waits for the response\n   * to resolve, and returns the response.\n   *\n   * If you want your response to have historical context, you must provide a valid `parentMessageId`.\n   *\n   * If you want to receive a stream of partial responses, use `opts.onProgress`.\n   *\n   * Set `debug: true` in the `ChatGPTAPI` constructor to log more info on the full prompt sent to the OpenAI chat completions API. You can override the `systemMessage` in `opts` to customize the assistant's instructions.\n   *\n   * @param message - The prompt message to send\n   * @param opts.parentMessageId - Optional ID of the previous message in the conversation (defaults to `undefined`)\n   * @param opts.conversationId - Optional ID of the conversation (defaults to `undefined`)\n   * @param opts.messageId - Optional ID of the message to send (defaults to a random UUID)\n   * @param opts.systemMessage - Optional override for the chat \"system message\" which acts as instructions to the model (defaults to the ChatGPT system message)\n   * @param opts.timeoutMs - Optional timeout in milliseconds (defaults to no timeout)\n   * @param opts.onProgress - Optional callback which will be invoked every time the partial response is updated\n   * @param opts.abortSignal - Optional callback used to abort the underlying `fetch` call using an [AbortController](https://developer.mozilla.org/en-US/docs/Web/API/AbortController)\n   * @param completionParams - Optional overrides to send to the [OpenAI chat completion API](https://platform.openai.com/docs/api-reference/chat/create). Options like `temperature` and `presence_penalty` can be tweaked to change the personality of the assistant.\n   *\n   * @returns The response from ChatGPT\n   */\n  async sendMessage(\n    text: string,\n    opts: types.SendMessageOptions = {}\n  ): Promise<types.ChatMessage> {\n    const {\n      parentMessageId,\n      messageId = uuidv4(),\n      timeoutMs,\n      onProgress,\n      stream = onProgress ? true : false,\n      completionParams,\n      conversationId\n    } = opts\n\n    let { abortSignal } = opts\n\n    let abortController: AbortController = null\n    if (timeoutMs && !abortSignal) {\n      abortController = new AbortController()\n      abortSignal = abortController.signal\n    }\n\n    const message: types.ChatMessage = {\n      role: 'user',\n      id: messageId,\n      conversationId,\n      parentMessageId,\n      text\n    }\n\n    const latestQuestion = message\n\n    const { messages, maxTokens, numTokens } = await this._buildMessages(\n      text,\n      opts\n    )\n\n    const result: types.ChatMessage = {\n      role: 'assistant',\n      id: uuidv4(),\n      conversationId,\n      parentMessageId: messageId,\n      text: ''\n    }\n\n    const responseP = new Promise<types.ChatMessage>(\n      async (resolve, reject) => {\n        const url = `${this._apiBaseUrl}/chat/completions`\n        const headers = {\n          'Content-Type': 'application/json',\n          Authorization: `Bearer ${this._apiKey}`\n        }\n        const body = {\n          max_tokens: maxTokens,\n          ...this._completionParams,\n          ...completionParams,\n          messages,\n          stream\n        }\n\n        // Support multiple organizations\n        // See https://platform.openai.com/docs/api-reference/authentication\n        if (this._apiOrg) {\n          headers['OpenAI-Organization'] = this._apiOrg\n        }\n\n        if (this._debug) {\n          console.log(`sendMessage (${numTokens} tokens)`, body)\n        }\n\n        if (stream) {\n          fetchSSE(\n            url,\n            {\n              method: 'POST',\n              headers,\n              body: JSON.stringify(body),\n              signal: abortSignal,\n              onMessage: (data: string) => {\n                if (data === '[DONE]') {\n                  result.text = result.text.trim()\n                  return resolve(result)\n                }\n\n                try {\n                  const response: types.openai.CreateChatCompletionDeltaResponse =\n                    JSON.parse(data)\n\n                  if (response.id) {\n                    result.id = response.id\n                  }\n\n                  if (response.choices?.length) {\n                    const delta = response.choices[0].delta\n                    result.delta = delta.content\n                    if (delta?.content) result.text += delta.content\n\n                    if (delta.role) {\n                      result.role = delta.role\n                    }\n\n                    result.detail = response\n                    onProgress?.(result)\n                  }\n                } catch (err) {\n                  console.warn('OpenAI stream SEE event unexpected error', err)\n                  return reject(err)\n                }\n              }\n            },\n            this._fetch\n          ).catch(reject)\n        } else {\n          try {\n            const res = await this._fetch(url, {\n              method: 'POST',\n              headers,\n              body: JSON.stringify(body),\n              signal: abortSignal\n            })\n\n            if (!res.ok) {\n              const reason = await res.text()\n              const msg = `OpenAI error ${\n                res.status || res.statusText\n              }: ${reason}`\n              const error = new types.ChatGPTError(msg, { cause: res })\n              error.statusCode = res.status\n              error.statusText = res.statusText\n              return reject(error)\n            }\n\n            const response: types.openai.CreateChatCompletionResponse =\n              await res.json()\n            if (this._debug) {\n              console.log(response)\n            }\n\n            if (response?.id) {\n              result.id = response.id\n            }\n\n            if (response?.choices?.length) {\n              const message = response.choices[0].message\n              result.text = message.content\n              if (message.role) {\n                result.role = message.role\n              }\n            } else {\n              const res = response as any\n              return reject(\n                new Error(\n                  `OpenAI error: ${\n                    res?.detail?.message || res?.detail || 'unknown'\n                  }`\n                )\n              )\n            }\n\n            result.detail = response\n\n            return resolve(result)\n          } catch (err) {\n            return reject(err)\n          }\n        }\n      }\n    ).then(async (message) => {\n      if (message.detail && !message.detail.usage) {\n        try {\n          const promptTokens = numTokens\n          const completionTokens = await this._getTokenCount(message.text)\n          message.detail.usage = {\n            prompt_tokens: promptTokens,\n            completion_tokens: completionTokens,\n            total_tokens: promptTokens + completionTokens,\n            estimated: true\n          }\n        } catch (err) {\n          // TODO: this should really never happen, but if it does,\n          // we should handle notify the user gracefully\n        }\n      }\n\n      return Promise.all([\n        this._upsertMessage(latestQuestion),\n        this._upsertMessage(message)\n      ]).then(() => message)\n    })\n\n    if (timeoutMs) {\n      if (abortController) {\n        // This will be called when a timeout occurs in order for us to forcibly\n        // ensure that the underlying HTTP request is aborted.\n        ;(responseP as any).cancel = () => {\n          abortController.abort()\n        }\n      }\n\n      return pTimeout(responseP, {\n        milliseconds: timeoutMs,\n        message: 'OpenAI timed out waiting for response'\n      })\n    } else {\n      return responseP\n    }\n  }\n\n  get apiKey(): string {\n    return this._apiKey\n  }\n\n  set apiKey(apiKey: string) {\n    this._apiKey = apiKey\n  }\n\n  get apiOrg(): string {\n    return this._apiOrg\n  }\n\n  set apiOrg(apiOrg: string) {\n    this._apiOrg = apiOrg\n  }\n\n  protected async _buildMessages(text: string, opts: types.SendMessageOptions) {\n    const { systemMessage = this._systemMessage } = opts\n    let { parentMessageId } = opts\n\n    const userLabel = USER_LABEL_DEFAULT\n    const assistantLabel = ASSISTANT_LABEL_DEFAULT\n\n    const maxNumTokens = this._maxModelTokens - this._maxResponseTokens\n    let messages: types.openai.ChatCompletionRequestMessage[] = []\n\n    if (systemMessage) {\n      messages.push({\n        role: 'system',\n        content: systemMessage\n      })\n    }\n\n    const systemMessageOffset = messages.length\n    let nextMessages = text\n      ? messages.concat([\n          {\n            role: 'user',\n            content: text,\n            name: opts.name\n          }\n        ])\n      : messages\n    let numTokens = 0\n\n    do {\n      const prompt = nextMessages\n        .reduce((prompt, message) => {\n          switch (message.role) {\n            case 'system':\n              return prompt.concat([`Instructions:\\n${message.content}`])\n            case 'user':\n              return prompt.concat([`${userLabel}:\\n${message.content}`])\n            default:\n              return prompt.concat([`${assistantLabel}:\\n${message.content}`])\n          }\n        }, [] as string[])\n        .join('\\n\\n')\n\n      const nextNumTokensEstimate = await this._getTokenCount(prompt)\n      const isValidPrompt = nextNumTokensEstimate <= maxNumTokens\n\n      if (prompt && !isValidPrompt) {\n        break\n      }\n\n      messages = nextMessages\n      numTokens = nextNumTokensEstimate\n\n      if (!isValidPrompt) {\n        break\n      }\n\n      if (!parentMessageId) {\n        break\n      }\n\n      const parentMessage = await this._getMessageById(parentMessageId)\n      if (!parentMessage) {\n        break\n      }\n\n      const parentMessageRole = parentMessage.role || 'user'\n\n      nextMessages = nextMessages.slice(0, systemMessageOffset).concat([\n        {\n          role: parentMessageRole,\n          content: parentMessage.text,\n          name: parentMessage.name\n        },\n        ...nextMessages.slice(systemMessageOffset)\n      ])\n\n      parentMessageId = parentMessage.parentMessageId\n    } while (true)\n\n    // Use up to 4096 tokens (prompt + response), but try to leave 1000 tokens\n    // for the response.\n    const maxTokens = Math.max(\n      1,\n      Math.min(this._maxModelTokens - numTokens, this._maxResponseTokens)\n    )\n\n    return { messages, maxTokens, numTokens }\n  }\n\n  protected async _getTokenCount(text: string) {\n    // TODO: use a better fix in the tokenizer\n    text = text.replace(/<\\|endoftext\\|>/g, '')\n\n    return tokenizer.encode(text).length\n  }\n\n  protected async _defaultGetMessageById(\n    id: string\n  ): Promise<types.ChatMessage> {\n    const res = await this._messageStore.get(id)\n    return res\n  }\n\n  protected async _defaultUpsertMessage(\n    message: types.ChatMessage\n  ): Promise<void> {\n    await this._messageStore.set(message.id, message)\n  }\n}\n","import { get_encoding } from '@dqbd/tiktoken'\n\n// TODO: make this configurable\nconst tokenizer = get_encoding('cl100k_base')\n\nexport function encode(input: string): Uint32Array {\n  return tokenizer.encode(input)\n}\n","import Keyv from 'keyv'\n\nexport type Role = 'user' | 'assistant' | 'system'\n\nexport type FetchFn = typeof fetch\n\nexport type ChatGPTAPIOptions = {\n  apiKey: string\n\n  /** @defaultValue `'https://api.openai.com'` **/\n  apiBaseUrl?: string\n\n  apiOrg?: string\n\n  /** @defaultValue `false` **/\n  debug?: boolean\n\n  completionParams?: Partial<\n    Omit<openai.CreateChatCompletionRequest, 'messages' | 'n' | 'stream'>\n  >\n\n  systemMessage?: string\n\n  /** @defaultValue `4096` **/\n  maxModelTokens?: number\n\n  /** @defaultValue `1000` **/\n  maxResponseTokens?: number\n\n  messageStore?: Keyv\n  getMessageById?: GetMessageByIdFunction\n  upsertMessage?: UpsertMessageFunction\n\n  fetch?: FetchFn\n}\n\nexport type SendMessageOptions = {\n  /** The name of a user in a multi-user chat. */\n  name?: string\n  parentMessageId?: string\n  conversationId?: string\n  messageId?: string\n  stream?: boolean\n  systemMessage?: string\n  timeoutMs?: number\n  onProgress?: (partialResponse: ChatMessage) => void\n  abortSignal?: AbortSignal\n  completionParams?: Partial<\n    Omit<openai.CreateChatCompletionRequest, 'messages' | 'n' | 'stream'>\n  >\n}\n\nexport type MessageActionType = 'next' | 'variant'\n\nexport type SendMessageBrowserOptions = {\n  conversationId?: string\n  parentMessageId?: string\n  messageId?: string\n  action?: MessageActionType\n  timeoutMs?: number\n  onProgress?: (partialResponse: ChatMessage) => void\n  abortSignal?: AbortSignal\n}\n\nexport interface ChatMessage {\n  id: string\n  text: string\n  role: Role\n  name?: string\n  delta?: string\n  detail?:\n    | openai.CreateChatCompletionResponse\n    | CreateChatCompletionStreamResponse\n\n  // relevant for both ChatGPTAPI and ChatGPTUnofficialProxyAPI\n  parentMessageId?: string\n\n  // only relevant for ChatGPTUnofficialProxyAPI (optional for ChatGPTAPI)\n  conversationId?: string\n}\n\nexport class ChatGPTError extends Error {\n  statusCode?: number\n  statusText?: string\n  isFinal?: boolean\n  accountId?: string\n}\n\n/** Returns a chat message from a store by it's ID (or null if not found). */\nexport type GetMessageByIdFunction = (id: string) => Promise<ChatMessage>\n\n/** Upserts a chat message to a store. */\nexport type UpsertMessageFunction = (message: ChatMessage) => Promise<void>\n\nexport interface CreateChatCompletionStreamResponse\n  extends openai.CreateChatCompletionDeltaResponse {\n  usage: CreateCompletionStreamResponseUsage\n}\n\nexport interface CreateCompletionStreamResponseUsage\n  extends openai.CreateCompletionResponseUsage {\n  estimated: true\n}\n\n/**\n * https://chat.openapi.com/backend-api/conversation\n */\nexport type ConversationJSONBody = {\n  /**\n   * The action to take\n   */\n  action: string\n\n  /**\n   * The ID of the conversation\n   */\n  conversation_id?: string\n\n  /**\n   * Prompts to provide\n   */\n  messages: Prompt[]\n\n  /**\n   * The model to use\n   */\n  model: string\n\n  /**\n   * The parent message ID\n   */\n  parent_message_id: string\n}\n\nexport type Prompt = {\n  /**\n   * The content of the prompt\n   */\n  content: PromptContent\n\n  /**\n   * The ID of the prompt\n   */\n  id: string\n\n  /**\n   * The role played in the prompt\n   */\n  role: Role\n}\n\nexport type ContentType = 'text'\n\nexport type PromptContent = {\n  /**\n   * The content type of the prompt\n   */\n  content_type: ContentType\n\n  /**\n   * The parts to the prompt\n   */\n  parts: string[]\n}\n\nexport type ConversationResponseEvent = {\n  message?: Message\n  conversation_id?: string\n  error?: string | null\n}\n\nexport type Message = {\n  id: string\n  content: MessageContent\n  role: Role\n  user: string | null\n  create_time: string | null\n  update_time: string | null\n  end_turn: null\n  weight: number\n  recipient: string\n  metadata: MessageMetadata\n}\n\nexport type MessageContent = {\n  content_type: string\n  parts: string[]\n}\n\nexport type MessageMetadata = any\n\nexport namespace openai {\n  export interface CreateChatCompletionDeltaResponse {\n    id: string\n    object: 'chat.completion.chunk'\n    created: number\n    model: string\n    choices: [\n      {\n        delta: {\n          role: Role\n          content?: string\n        }\n        index: number\n        finish_reason: string | null\n      }\n    ]\n  }\n\n  /**\n   *\n   * @export\n   * @interface ChatCompletionRequestMessage\n   */\n  export interface ChatCompletionRequestMessage {\n    /**\n     * The role of the author of this message.\n     * @type {string}\n     * @memberof ChatCompletionRequestMessage\n     */\n    role: ChatCompletionRequestMessageRoleEnum\n    /**\n     * The contents of the message\n     * @type {string}\n     * @memberof ChatCompletionRequestMessage\n     */\n    content: string\n    /**\n     * The name of the user in a multi-user chat\n     * @type {string}\n     * @memberof ChatCompletionRequestMessage\n     */\n    name?: string\n  }\n  export declare const ChatCompletionRequestMessageRoleEnum: {\n    readonly System: 'system'\n    readonly User: 'user'\n    readonly Assistant: 'assistant'\n  }\n  export declare type ChatCompletionRequestMessageRoleEnum =\n    (typeof ChatCompletionRequestMessageRoleEnum)[keyof typeof ChatCompletionRequestMessageRoleEnum]\n  /**\n   *\n   * @export\n   * @interface ChatCompletionResponseMessage\n   */\n  export interface ChatCompletionResponseMessage {\n    /**\n     * The role of the author of this message.\n     * @type {string}\n     * @memberof ChatCompletionResponseMessage\n     */\n    role: ChatCompletionResponseMessageRoleEnum\n    /**\n     * The contents of the message\n     * @type {string}\n     * @memberof ChatCompletionResponseMessage\n     */\n    content: string\n  }\n  export declare const ChatCompletionResponseMessageRoleEnum: {\n    readonly System: 'system'\n    readonly User: 'user'\n    readonly Assistant: 'assistant'\n  }\n  export declare type ChatCompletionResponseMessageRoleEnum =\n    (typeof ChatCompletionResponseMessageRoleEnum)[keyof typeof ChatCompletionResponseMessageRoleEnum]\n  /**\n   *\n   * @export\n   * @interface CreateChatCompletionRequest\n   */\n  export interface CreateChatCompletionRequest {\n    /**\n     * ID of the model to use. Currently, only `gpt-3.5-turbo` and `gpt-3.5-turbo-0301` are supported.\n     * @type {string}\n     * @memberof CreateChatCompletionRequest\n     */\n    model: string\n    /**\n     * The messages to generate chat completions for, in the [chat format](/docs/guides/chat/introduction).\n     * @type {Array<ChatCompletionRequestMessage>}\n     * @memberof CreateChatCompletionRequest\n     */\n    messages: Array<ChatCompletionRequestMessage>\n    /**\n     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or `top_p` but not both.\n     * @type {number}\n     * @memberof CreateChatCompletionRequest\n     */\n    temperature?: number | null\n    /**\n     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or `temperature` but not both.\n     * @type {number}\n     * @memberof CreateChatCompletionRequest\n     */\n    top_p?: number | null\n    /**\n     * How many chat completion choices to generate for each input message.\n     * @type {number}\n     * @memberof CreateChatCompletionRequest\n     */\n    n?: number | null\n    /**\n     * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message.\n     * @type {boolean}\n     * @memberof CreateChatCompletionRequest\n     */\n    stream?: boolean | null\n    /**\n     *\n     * @type {CreateChatCompletionRequestStop}\n     * @memberof CreateChatCompletionRequest\n     */\n    stop?: CreateChatCompletionRequestStop\n    /**\n     * The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens).\n     * @type {number}\n     * @memberof CreateChatCompletionRequest\n     */\n    max_tokens?: number\n    /**\n     * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model\\'s likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\n     * @type {number}\n     * @memberof CreateChatCompletionRequest\n     */\n    presence_penalty?: number | null\n    /**\n     * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model\\'s likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\n     * @type {number}\n     * @memberof CreateChatCompletionRequest\n     */\n    frequency_penalty?: number | null\n    /**\n     * Modify the likelihood of specified tokens appearing in the completion.  Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n     * @type {object}\n     * @memberof CreateChatCompletionRequest\n     */\n    logit_bias?: object | null\n    /**\n     * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).\n     * @type {string}\n     * @memberof CreateChatCompletionRequest\n     */\n    user?: string\n  }\n  /**\n   * @type CreateChatCompletionRequestStop\n   * Up to 4 sequences where the API will stop generating further tokens.\n   * @export\n   */\n  export declare type CreateChatCompletionRequestStop = Array<string> | string\n  /**\n   *\n   * @export\n   * @interface CreateChatCompletionResponse\n   */\n  export interface CreateChatCompletionResponse {\n    /**\n     *\n     * @type {string}\n     * @memberof CreateChatCompletionResponse\n     */\n    id: string\n    /**\n     *\n     * @type {string}\n     * @memberof CreateChatCompletionResponse\n     */\n    object: string\n    /**\n     *\n     * @type {number}\n     * @memberof CreateChatCompletionResponse\n     */\n    created: number\n    /**\n     *\n     * @type {string}\n     * @memberof CreateChatCompletionResponse\n     */\n    model: string\n    /**\n     *\n     * @type {Array<CreateChatCompletionResponseChoicesInner>}\n     * @memberof CreateChatCompletionResponse\n     */\n    choices: Array<CreateChatCompletionResponseChoicesInner>\n    /**\n     *\n     * @type {CreateCompletionResponseUsage}\n     * @memberof CreateChatCompletionResponse\n     */\n    usage?: CreateCompletionResponseUsage\n  }\n  /**\n   *\n   * @export\n   * @interface CreateChatCompletionResponseChoicesInner\n   */\n  export interface CreateChatCompletionResponseChoicesInner {\n    /**\n     *\n     * @type {number}\n     * @memberof CreateChatCompletionResponseChoicesInner\n     */\n    index?: number\n    /**\n     *\n     * @type {ChatCompletionResponseMessage}\n     * @memberof CreateChatCompletionResponseChoicesInner\n     */\n    message?: ChatCompletionResponseMessage\n    /**\n     *\n     * @type {string}\n     * @memberof CreateChatCompletionResponseChoicesInner\n     */\n    finish_reason?: string\n  }\n  /**\n   *\n   * @export\n   * @interface CreateCompletionResponseUsage\n   */\n  export interface CreateCompletionResponseUsage {\n    /**\n     *\n     * @type {number}\n     * @memberof CreateCompletionResponseUsage\n     */\n    prompt_tokens: number\n    /**\n     *\n     * @type {number}\n     * @memberof CreateCompletionResponseUsage\n     */\n    completion_tokens: number\n    /**\n     *\n     * @type {number}\n     * @memberof CreateCompletionResponseUsage\n     */\n    total_tokens: number\n  }\n}\n","/// <reference lib=\"dom\" />\n\nconst fetch = globalThis.fetch\n\nexport { fetch }\n","import { createParser } from 'eventsource-parser'\n\nimport * as types from './types'\nimport { fetch as globalFetch } from './fetch'\nimport { streamAsyncIterable } from './stream-async-iterable'\n\nexport async function fetchSSE(\n  url: string,\n  options: Parameters<typeof fetch>[1] & {\n    onMessage: (data: string) => void\n    onError?: (error: any) => void\n  },\n  fetch: types.FetchFn = globalFetch\n) {\n  const { onMessage, onError, ...fetchOptions } = options\n  const res = await fetch(url, fetchOptions)\n  if (!res.ok) {\n    let reason: string\n\n    try {\n      reason = await res.text()\n    } catch (err) {\n      reason = res.statusText\n    }\n\n    const msg = `ChatGPT error ${res.status}: ${reason}`\n    const error = new types.ChatGPTError(msg, { cause: res })\n    error.statusCode = res.status\n    error.statusText = res.statusText\n    throw error\n  }\n\n  const parser = createParser((event) => {\n    if (event.type === 'event') {\n      onMessage(event.data)\n    }\n  })\n\n  // handle special response errors\n  const feed = (chunk: string) => {\n    let response = null\n\n    try {\n      response = JSON.parse(chunk)\n    } catch {\n      // ignore\n    }\n\n    if (response?.detail?.type === 'invalid_request_error') {\n      const msg = `ChatGPT error ${response.detail.message}: ${response.detail.code} (${response.detail.type})`\n      const error = new types.ChatGPTError(msg, { cause: response })\n      error.statusCode = response.detail.code\n      error.statusText = response.detail.message\n\n      if (onError) {\n        onError(error)\n      } else {\n        console.error(error)\n      }\n\n      // don't feed to the event parser\n      return\n    }\n\n    parser.feed(chunk)\n  }\n\n  if (!res.body.getReader) {\n    // Vercel polyfills `fetch` with `node-fetch`, which doesn't conform to\n    // web standards, so this is a workaround...\n    const body: NodeJS.ReadableStream = res.body as any\n\n    if (!body.on || !body.read) {\n      throw new types.ChatGPTError('unsupported \"fetch\" implementation')\n    }\n\n    body.on('readable', () => {\n      let chunk: string | Buffer\n      while (null !== (chunk = body.read())) {\n        feed(chunk.toString())\n      }\n    })\n  } else {\n    for await (const chunk of streamAsyncIterable(res.body)) {\n      const str = new TextDecoder().decode(chunk)\n      feed(str)\n    }\n  }\n}\n","export async function* streamAsyncIterable<T>(stream: ReadableStream<T>) {\n  const reader = stream.getReader()\n  try {\n    while (true) {\n      const { done, value } = await reader.read()\n      if (done) {\n        return\n      }\n      yield value\n    }\n  } finally {\n    reader.releaseLock()\n  }\n}\n","import pTimeout from 'p-timeout'\nimport { v4 as uuidv4 } from 'uuid'\n\nimport * as types from './types'\nimport { fetch as globalFetch } from './fetch'\nimport { fetchSSE } from './fetch-sse'\nimport { isValidUUIDv4 } from './utils'\n\nexport class ChatGPTUnofficialProxyAPI {\n  protected _accessToken: string\n  protected _apiReverseProxyUrl: string\n  protected _debug: boolean\n  protected _model: string\n  protected _headers: Record<string, string>\n  protected _fetch: types.FetchFn\n\n  /**\n   * @param fetch - Optional override for the `fetch` implementation to use. Defaults to the global `fetch` function.\n   */\n  constructor(opts: {\n    accessToken: string\n\n    /** @defaultValue `https://bypass.duti.tech/api/conversation` **/\n    apiReverseProxyUrl?: string\n\n    /** @defaultValue `text-davinci-002-render-sha` **/\n    model?: string\n\n    /** @defaultValue `false` **/\n    debug?: boolean\n\n    /** @defaultValue `undefined` **/\n    headers?: Record<string, string>\n\n    fetch?: types.FetchFn\n  }) {\n    const {\n      accessToken,\n      apiReverseProxyUrl = 'https://bypass.duti.tech/api/conversation',\n      model = 'text-davinci-002-render-sha',\n      debug = false,\n      headers,\n      fetch = globalFetch\n    } = opts\n\n    this._accessToken = accessToken\n    this._apiReverseProxyUrl = apiReverseProxyUrl\n    this._debug = !!debug\n    this._model = model\n    this._fetch = fetch\n    this._headers = headers\n\n    if (!this._accessToken) {\n      throw new Error('ChatGPT invalid accessToken')\n    }\n\n    if (!this._fetch) {\n      throw new Error('Invalid environment; fetch is not defined')\n    }\n\n    if (typeof this._fetch !== 'function') {\n      throw new Error('Invalid \"fetch\" is not a function')\n    }\n  }\n\n  get accessToken(): string {\n    return this._accessToken\n  }\n\n  set accessToken(value: string) {\n    this._accessToken = value\n  }\n\n  /**\n   * Sends a message to ChatGPT, waits for the response to resolve, and returns\n   * the response.\n   *\n   * If you want your response to have historical context, you must provide a valid `parentMessageId`.\n   *\n   * If you want to receive a stream of partial responses, use `opts.onProgress`.\n   * If you want to receive the full response, including message and conversation IDs,\n   * you can use `opts.onConversationResponse` or use the `ChatGPTAPI.getConversation`\n   * helper.\n   *\n   * Set `debug: true` in the `ChatGPTAPI` constructor to log more info on the full prompt sent to the OpenAI completions API. You can override the `promptPrefix` and `promptSuffix` in `opts` to customize the prompt.\n   *\n   * @param message - The prompt message to send\n   * @param opts.conversationId - Optional ID of a conversation to continue (defaults to a random UUID)\n   * @param opts.parentMessageId - Optional ID of the previous message in the conversation (defaults to `undefined`)\n   * @param opts.messageId - Optional ID of the message to send (defaults to a random UUID)\n   * @param opts.timeoutMs - Optional timeout in milliseconds (defaults to no timeout)\n   * @param opts.onProgress - Optional callback which will be invoked every time the partial response is updated\n   * @param opts.abortSignal - Optional callback used to abort the underlying `fetch` call using an [AbortController](https://developer.mozilla.org/en-US/docs/Web/API/AbortController)\n   *\n   * @returns The response from ChatGPT\n   */\n  async sendMessage(\n    text: string,\n    opts: types.SendMessageBrowserOptions = {}\n  ): Promise<types.ChatMessage> {\n    if (!!opts.conversationId !== !!opts.parentMessageId) {\n      throw new Error(\n        'ChatGPTUnofficialProxyAPI.sendMessage: conversationId and parentMessageId must both be set or both be undefined'\n      )\n    }\n\n    if (opts.conversationId && !isValidUUIDv4(opts.conversationId)) {\n      throw new Error(\n        'ChatGPTUnofficialProxyAPI.sendMessage: conversationId is not a valid v4 UUID'\n      )\n    }\n\n    if (opts.parentMessageId && !isValidUUIDv4(opts.parentMessageId)) {\n      throw new Error(\n        'ChatGPTUnofficialProxyAPI.sendMessage: parentMessageId is not a valid v4 UUID'\n      )\n    }\n\n    if (opts.messageId && !isValidUUIDv4(opts.messageId)) {\n      throw new Error(\n        'ChatGPTUnofficialProxyAPI.sendMessage: messageId is not a valid v4 UUID'\n      )\n    }\n\n    const {\n      conversationId,\n      parentMessageId = uuidv4(),\n      messageId = uuidv4(),\n      action = 'next',\n      timeoutMs,\n      onProgress\n    } = opts\n\n    let { abortSignal } = opts\n\n    let abortController: AbortController = null\n    if (timeoutMs && !abortSignal) {\n      abortController = new AbortController()\n      abortSignal = abortController.signal\n    }\n\n    const body: types.ConversationJSONBody = {\n      action,\n      messages: [\n        {\n          id: messageId,\n          role: 'user',\n          content: {\n            content_type: 'text',\n            parts: [text]\n          }\n        }\n      ],\n      model: this._model,\n      parent_message_id: parentMessageId\n    }\n\n    if (conversationId) {\n      body.conversation_id = conversationId\n    }\n\n    const result: types.ChatMessage = {\n      role: 'assistant',\n      id: uuidv4(),\n      parentMessageId: messageId,\n      conversationId,\n      text: ''\n    }\n\n    const responseP = new Promise<types.ChatMessage>((resolve, reject) => {\n      const url = this._apiReverseProxyUrl\n      const headers = {\n        ...this._headers,\n        Authorization: `Bearer ${this._accessToken}`,\n        Accept: 'text/event-stream',\n        'Content-Type': 'application/json'\n      }\n\n      if (this._debug) {\n        console.log('POST', url, { body, headers })\n      }\n\n      fetchSSE(\n        url,\n        {\n          method: 'POST',\n          headers,\n          body: JSON.stringify(body),\n          signal: abortSignal,\n          onMessage: (data: string) => {\n            if (data === '[DONE]') {\n              return resolve(result)\n            }\n\n            try {\n              const convoResponseEvent: types.ConversationResponseEvent =\n                JSON.parse(data)\n              if (convoResponseEvent.conversation_id) {\n                result.conversationId = convoResponseEvent.conversation_id\n              }\n\n              if (convoResponseEvent.message?.id) {\n                result.id = convoResponseEvent.message.id\n              }\n\n              const message = convoResponseEvent.message\n              // console.log('event', JSON.stringify(convoResponseEvent, null, 2))\n\n              if (message) {\n                let text = message?.content?.parts?.[0]\n\n                if (text) {\n                  result.text = text\n\n                  if (onProgress) {\n                    onProgress(result)\n                  }\n                }\n              }\n            } catch (err) {\n              reject(err)\n            }\n          },\n          onError: (err) => {\n            reject(err)\n          }\n        },\n        this._fetch\n      ).catch((err) => {\n        const errMessageL = err.toString().toLowerCase()\n\n        if (\n          result.text &&\n          (errMessageL === 'error: typeerror: terminated' ||\n            errMessageL === 'typeerror: terminated')\n        ) {\n          // OpenAI sometimes forcefully terminates the socket from their end before\n          // the HTTP request has resolved cleanly. In my testing, these cases tend to\n          // happen when OpenAI has already send the last `response`, so we can ignore\n          // the `fetch` error in this case.\n          return resolve(result)\n        } else {\n          return reject(err)\n        }\n      })\n    })\n\n    if (timeoutMs) {\n      if (abortController) {\n        // This will be called when a timeout occurs in order for us to forcibly\n        // ensure that the underlying HTTP request is aborted.\n        ;(responseP as any).cancel = () => {\n          abortController.abort()\n        }\n      }\n\n      return pTimeout(responseP, {\n        milliseconds: timeoutMs,\n        message: 'ChatGPT timed out waiting for response'\n      })\n    } else {\n      return responseP\n    }\n  }\n}\n","const uuidv4Re =\n  /^[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}$/i\n\nexport function isValidUUIDv4(str: string): boolean {\n  return str && uuidv4Re.test(str)\n}\n"],"mappings":";AAAA,OAAOA,IAAA,MAAU;AACjB,OAAOC,QAAA,MAAc;AACrB,OAAOC,QAAA,MAAc;AACrB,SAASC,EAAA,IAAMC,MAAA,QAAc;;;ACH7B,SAASC,YAAA,QAAoB;AAG7B,IAAMC,SAAA,GAAYD,YAAA,CAAa,aAAa;AAErC,SAASE,OAAOC,KAAA,EAA4B;EACjD,OAAOF,SAAA,CAAUC,MAAA,CAAOC,KAAK;AAC/B;;;AC0EO,IAAMC,YAAA,GAAN,cAA2BC,KAAA,CAAM,EAKxC;AAyGO,IAAUC,MAAA;AAAA,CAAAC,OAAA,IAAV,IAAUD,MAAA,KAAAA,MAAA;;;AC7LjB,IAAME,KAAA,GAAQC,UAAA,CAAWD,KAAA;;;ACFzB,SAASE,YAAA,QAAoB;;;ACA7B,gBAAuBC,oBAAuBC,MAAA,EAA2B;EACvE,MAAMC,MAAA,GAASD,MAAA,CAAOE,SAAA,EAAU;EAChC,IAAI;IACF,OAAO,MAAM;MACX,MAAM;QAAEC,IAAA;QAAMC;MAAM,IAAI,MAAMH,MAAA,CAAOI,IAAA,EAAK;MAC1C,IAAIF,IAAA,EAAM;QACR;MACF;MACA,MAAMC,KAAA;IACR;EACF,UAAE;IACAH,MAAA,CAAOK,WAAA,EAAY;EACrB;AACF;;;ADPA,eAAsBC,SACpBC,GAAA,EACAC,OAAA,EAKA;EAAA,IADAC,MAAA,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAuBf,KAAA;EAEvB,MAAM;IAAEkB,SAAA;IAAWC,OAAA;IAAS,GAAGC;EAAa,IAAIP,OAAA;EAChD,MAAMQ,GAAA,GAAM,MAAMP,MAAA,CAAMF,GAAA,EAAKQ,YAAY;EACzC,IAAI,CAACC,GAAA,CAAIC,EAAA,EAAI;IACX,IAAIC,MAAA;IAEJ,IAAI;MACFA,MAAA,GAAS,MAAMF,GAAA,CAAIG,IAAA,EAAK;IAC1B,SAASC,GAAA,EAAP;MACAF,MAAA,GAASF,GAAA,CAAIK,UAAA;IACf;IAEA,MAAMC,GAAA,GAAM,iBAAiBN,GAAA,CAAIO,MAAA,KAAWL,MAAA;IAC5C,MAAMM,KAAA,GAAQ,IAAUjC,YAAA,CAAa+B,GAAA,EAAK;MAAEG,KAAA,EAAOT;IAAI,CAAC;IACxDQ,KAAA,CAAME,UAAA,GAAaV,GAAA,CAAIO,MAAA;IACvBC,KAAA,CAAMH,UAAA,GAAaL,GAAA,CAAIK,UAAA;IACvB,MAAMG,KAAA;EACR;EAEA,MAAMG,MAAA,GAAS9B,YAAA,CAAc+B,KAAA,IAAU;IACrC,IAAIA,KAAA,CAAMC,IAAA,KAAS,SAAS;MAC1BhB,SAAA,CAAUe,KAAA,CAAME,IAAI;IACtB;EACF,CAAC;EAGD,MAAMC,IAAA,GAAQC,KAAA,IAAkB;IAvClC,IAAAC,EAAA;IAwCI,IAAIC,QAAA,GAAW;IAEf,IAAI;MACFA,QAAA,GAAWC,IAAA,CAAKC,KAAA,CAAMJ,KAAK;IAC7B,QAAE,CAEF;IAEA,MAAIC,EAAA,GAAAC,QAAA,oBAAAA,QAAA,CAAUG,MAAA,KAAV,gBAAAJ,EAAA,CAAkBJ,IAAA,MAAS,yBAAyB;MACtD,MAAMP,GAAA,GAAM,iBAAiBY,QAAA,CAASG,MAAA,CAAOC,OAAA,KAAYJ,QAAA,CAASG,MAAA,CAAOE,IAAA,KAASL,QAAA,CAASG,MAAA,CAAOR,IAAA;MAClG,MAAML,KAAA,GAAQ,IAAUjC,YAAA,CAAa+B,GAAA,EAAK;QAAEG,KAAA,EAAOS;MAAS,CAAC;MAC7DV,KAAA,CAAME,UAAA,GAAaQ,QAAA,CAASG,MAAA,CAAOE,IAAA;MACnCf,KAAA,CAAMH,UAAA,GAAaa,QAAA,CAASG,MAAA,CAAOC,OAAA;MAEnC,IAAIxB,OAAA,EAAS;QACXA,OAAA,CAAQU,KAAK;MACf,OAAO;QACLgB,OAAA,CAAQhB,KAAA,CAAMA,KAAK;MACrB;MAGA;IACF;IAEAG,MAAA,CAAOI,IAAA,CAAKC,KAAK;EACnB;EAEA,IAAI,CAAChB,GAAA,CAAIyB,IAAA,CAAKxC,SAAA,EAAW;IAGvB,MAAMwC,IAAA,GAA8BzB,GAAA,CAAIyB,IAAA;IAExC,IAAI,CAACA,IAAA,CAAKC,EAAA,IAAM,CAACD,IAAA,CAAKrC,IAAA,EAAM;MAC1B,MAAM,IAAUb,YAAA,CAAa,oCAAoC;IACnE;IAEAkD,IAAA,CAAKC,EAAA,CAAG,YAAY,MAAM;MACxB,IAAIV,KAAA;MACJ,OAAO,UAAUA,KAAA,GAAQS,IAAA,CAAKrC,IAAA,EAAK,GAAI;QACrC2B,IAAA,CAAKC,KAAA,CAAMW,QAAA,EAAU;MACvB;IACF,CAAC;EACH,OAAO;IACL,iBAAiBX,KAAA,IAASlC,mBAAA,CAAoBkB,GAAA,CAAIyB,IAAI,GAAG;MACvD,MAAMG,GAAA,GAAM,IAAIC,WAAA,EAAY,CAAEC,MAAA,CAAOd,KAAK;MAC1CD,IAAA,CAAKa,GAAG;IACV;EACF;AACF;;;AJ9EA,IAAMG,aAAA,GAAgB;AAEtB,IAAMC,kBAAA,GAAqB;AAC3B,IAAMC,uBAAA,GAA0B;AAEzB,IAAMC,UAAA,GAAN,MAAiB;EAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;EAmCtBC,YAAYC,IAAA,EAA+B;IACzC,MAAM;MACJC,MAAA;MACAC,MAAA;MACAC,UAAA,GAAa;MACbC,KAAA,GAAQ;MACRC,YAAA;MACAC,gBAAA;MACAC,aAAA;MACAC,cAAA,GAAiB;MACjBC,iBAAA,GAAoB;MACpBC,cAAA;MACAC,aAAA;MACApE,KAAA,EAAAc,MAAA,GAAQd;IACV,IAAIyD,IAAA;IAEJ,KAAKY,OAAA,GAAUX,MAAA;IACf,KAAKY,OAAA,GAAUX,MAAA;IACf,KAAKY,WAAA,GAAcX,UAAA;IACnB,KAAKY,MAAA,GAAS,CAAC,CAACX,KAAA;IAChB,KAAKY,MAAA,GAAS3D,MAAA;IAEd,KAAK4D,iBAAA,GAAoB;MACvBC,KAAA,EAAOvB,aAAA;MACPwB,WAAA,EAAa;MACbC,KAAA,EAAO;MACPC,gBAAA,EAAkB;MAClB,GAAGf;IACL;IAEA,KAAKgB,cAAA,GAAiBf,aAAA;IAEtB,IAAI,KAAKe,cAAA,KAAmB,QAAW;MACrC,MAAMC,WAAA,GAAc,mBAAIC,IAAA,EAAK,CAAEC,WAAA,EAAY,CAAEC,KAAA,CAAM,GAAG,EAAE,CAAC;MACzD,KAAKJ,cAAA,GAAiB;AAAA;AAAA,gBAA4IC,WAAA;IACpK;IAEA,KAAKI,eAAA,GAAkBnB,cAAA;IACvB,KAAKoB,kBAAA,GAAqBnB,iBAAA;IAE1B,KAAKoB,eAAA,GAAkBnB,cAAA,IAAkB,KAAKoB,sBAAA;IAC9C,KAAKC,cAAA,GAAiBpB,aAAA,IAAiB,KAAKqB,qBAAA;IAE5C,IAAI3B,YAAA,EAAc;MAChB,KAAK4B,aAAA,GAAgB5B,YAAA;IACvB,OAAO;MACL,KAAK4B,aAAA,GAAgB,IAAIvG,IAAA,CAA6B;QACpDwG,KAAA,EAAO,IAAItG,QAAA,CAAoC;UAAEuG,OAAA,EAAS;QAAM,CAAC;MACnE,CAAC;IACH;IAEA,IAAI,CAAC,KAAKvB,OAAA,EAAS;MACjB,MAAM,IAAIxE,KAAA,CAAM,gCAAgC;IAClD;IAEA,IAAI,CAAC,KAAK4E,MAAA,EAAQ;MAChB,MAAM,IAAI5E,KAAA,CAAM,2CAA2C;IAC7D;IAEA,IAAI,OAAO,KAAK4E,MAAA,KAAW,YAAY;MACrC,MAAM,IAAI5E,KAAA,CAAM,mCAAmC;IACrD;EACF;EAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;EAwBA,MAAMgG,YACJrE,IAAA,EAE4B;IAAA,IAD5BiC,IAAA,GAAA1C,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAiC,CAAC;IAElC,MAAM;MACJ+E,eAAA;MACAC,SAAA,GAAYxG,MAAA,EAAO;MACnByG,SAAA;MACAC,UAAA;MACA7F,MAAA,GAAS6F,UAAA,GAAa,OAAO;MAC7BlC,gBAAA;MACAmC;IACF,IAAIzC,IAAA;IAEJ,IAAI;MAAE0C;IAAY,IAAI1C,IAAA;IAEtB,IAAI2C,eAAA,GAAmC;IACvC,IAAIJ,SAAA,IAAa,CAACG,WAAA,EAAa;MAC7BC,eAAA,GAAkB,IAAIC,eAAA,EAAgB;MACtCF,WAAA,GAAcC,eAAA,CAAgBE,MAAA;IAChC;IAEA,MAAM3D,OAAA,GAA6B;MACjC4D,IAAA,EAAM;MACNC,EAAA,EAAIT,SAAA;MACJG,cAAA;MACAJ,eAAA;MACAtE;IACF;IAEA,MAAMiF,cAAA,GAAiB9D,OAAA;IAEvB,MAAM;MAAE+D,QAAA;MAAUC,SAAA;MAAWC;IAAU,IAAI,MAAM,KAAKC,cAAA,CACpDrF,IAAA,EACAiC,IAAA,CACF;IAEA,MAAMqD,MAAA,GAA4B;MAChCP,IAAA,EAAM;MACNC,EAAA,EAAIjH,MAAA,EAAO;MACX2G,cAAA;MACAJ,eAAA,EAAiBC,SAAA;MACjBvE,IAAA,EAAM;IACR;IAEA,MAAMuF,SAAA,GAAY,IAAIC,OAAA,CACpB,OAAOC,OAAA,EAASC,MAAA,KAAW;MAtLjC,IAAA5E,EAAA,EAAA6E,EAAA;MAuLQ,MAAMvG,GAAA,GAAM,GAAG,KAAK2D,WAAA;MACpB,MAAM6C,OAAA,GAAU;QACd,gBAAgB;QAChBC,aAAA,EAAe,UAAU,KAAKhD,OAAA;MAChC;MACA,MAAMvB,IAAA,GAAO;QACXwE,UAAA,EAAYX,SAAA;QACZ,GAAG,KAAKjC,iBAAA;QACR,GAAGX,gBAAA;QACH2C,QAAA;QACAtG;MACF;MAIA,IAAI,KAAKkE,OAAA,EAAS;QAChB8C,OAAA,CAAQ,qBAAqB,IAAI,KAAK9C,OAAA;MACxC;MAEA,IAAI,KAAKE,MAAA,EAAQ;QACf3B,OAAA,CAAQ0E,GAAA,CAAI,gBAAgBX,SAAA,YAAqB9D,IAAI;MACvD;MAEA,IAAI1C,MAAA,EAAQ;QACVO,QAAA,CACEC,GAAA,EACA;UACE4G,MAAA,EAAQ;UACRJ,OAAA;UACAtE,IAAA,EAAMN,IAAA,CAAKiF,SAAA,CAAU3E,IAAI;UACzBwD,MAAA,EAAQH,WAAA;UACRjF,SAAA,EAAYiB,IAAA,IAAiB;YAtN3C,IAAAuF,GAAA;YAuNgB,IAAIvF,IAAA,KAAS,UAAU;cACrB2E,MAAA,CAAOtF,IAAA,GAAOsF,MAAA,CAAOtF,IAAA,CAAKmG,IAAA,EAAK;cAC/B,OAAOV,OAAA,CAAQH,MAAM;YACvB;YAEA,IAAI;cACF,MAAMvE,QAAA,GACJC,IAAA,CAAKC,KAAA,CAAMN,IAAI;cAEjB,IAAII,QAAA,CAASiE,EAAA,EAAI;gBACfM,MAAA,CAAON,EAAA,GAAKjE,QAAA,CAASiE,EAAA;cACvB;cAEA,KAAIkB,GAAA,GAAAnF,QAAA,CAASqF,OAAA,KAAT,gBAAAF,GAAA,CAAkB1G,MAAA,EAAQ;gBAC5B,MAAM6G,KAAA,GAAQtF,QAAA,CAASqF,OAAA,CAAQ,CAAC,EAAEC,KAAA;gBAClCf,MAAA,CAAOe,KAAA,GAAQA,KAAA,CAAMC,OAAA;gBACrB,IAAID,KAAA,oBAAAA,KAAA,CAAOC,OAAA,EAAShB,MAAA,CAAOtF,IAAA,IAAQqG,KAAA,CAAMC,OAAA;gBAEzC,IAAID,KAAA,CAAMtB,IAAA,EAAM;kBACdO,MAAA,CAAOP,IAAA,GAAOsB,KAAA,CAAMtB,IAAA;gBACtB;gBAEAO,MAAA,CAAOpE,MAAA,GAASH,QAAA;gBAChB0D,UAAA,oBAAAA,UAAA,CAAaa,MAAA;cACf;YACF,SAASrF,GAAA,EAAP;cACAoB,OAAA,CAAQkF,IAAA,CAAK,4CAA4CtG,GAAG;cAC5D,OAAOyF,MAAA,CAAOzF,GAAG;YACnB;UACF;QACF,GACA,KAAKgD,MAAA,CACP,CAAEuD,KAAA,CAAMd,MAAM;MAChB,OAAO;QACL,IAAI;UACF,MAAM7F,GAAA,GAAM,MAAM,KAAKoD,MAAA,CAAO7D,GAAA,EAAK;YACjC4G,MAAA,EAAQ;YACRJ,OAAA;YACAtE,IAAA,EAAMN,IAAA,CAAKiF,SAAA,CAAU3E,IAAI;YACzBwD,MAAA,EAAQH;UACV,CAAC;UAED,IAAI,CAAC9E,GAAA,CAAIC,EAAA,EAAI;YACX,MAAMC,MAAA,GAAS,MAAMF,GAAA,CAAIG,IAAA,EAAK;YAC9B,MAAMG,GAAA,GAAM,gBACVN,GAAA,CAAIO,MAAA,IAAUP,GAAA,CAAIK,UAAA,KACfH,MAAA;YACL,MAAMM,KAAA,GAAQ,IAAUjC,YAAA,CAAa+B,GAAA,EAAK;cAAEG,KAAA,EAAOT;YAAI,CAAC;YACxDQ,KAAA,CAAME,UAAA,GAAaV,GAAA,CAAIO,MAAA;YACvBC,KAAA,CAAMH,UAAA,GAAaL,GAAA,CAAIK,UAAA;YACvB,OAAOwF,MAAA,CAAOrF,KAAK;UACrB;UAEA,MAAMU,QAAA,GACJ,MAAMlB,GAAA,CAAI4G,IAAA,EAAK;UACjB,IAAI,KAAKzD,MAAA,EAAQ;YACf3B,OAAA,CAAQ0E,GAAA,CAAIhF,QAAQ;UACtB;UAEA,IAAIA,QAAA,oBAAAA,QAAA,CAAUiE,EAAA,EAAI;YAChBM,MAAA,CAAON,EAAA,GAAKjE,QAAA,CAASiE,EAAA;UACvB;UAEA,KAAIlE,EAAA,GAAAC,QAAA,oBAAAA,QAAA,CAAUqF,OAAA,KAAV,gBAAAtF,EAAA,CAAmBtB,MAAA,EAAQ;YAC7B,MAAMkH,QAAA,GAAU3F,QAAA,CAASqF,OAAA,CAAQ,CAAC,EAAEjF,OAAA;YACpCmE,MAAA,CAAOtF,IAAA,GAAO0G,QAAA,CAAQJ,OAAA;YACtB,IAAII,QAAA,CAAQ3B,IAAA,EAAM;cAChBO,MAAA,CAAOP,IAAA,GAAO2B,QAAA,CAAQ3B,IAAA;YACxB;UACF,OAAO;YACL,MAAM4B,IAAA,GAAM5F,QAAA;YACZ,OAAO2E,MAAA,CACL,IAAIrH,KAAA,CACF,mBACEsH,EAAA,GAAAgB,IAAA,oBAAAA,IAAA,CAAKzF,MAAA,KAAL,gBAAAyE,EAAA,CAAaxE,OAAA,MAAWwF,IAAA,oBAAAA,IAAA,CAAKzF,MAAA,KAAU,YAE3C,CACF;UACF;UAEAoE,MAAA,CAAOpE,MAAA,GAASH,QAAA;UAEhB,OAAO0E,OAAA,CAAQH,MAAM;QACvB,SAASrF,GAAA,EAAP;UACA,OAAOyF,MAAA,CAAOzF,GAAG;QACnB;MACF;IACF,EACF,CAAE2G,IAAA,CAAK,MAAOF,QAAA,IAAY;MACxB,IAAIA,QAAA,CAAQxF,MAAA,IAAU,CAACwF,QAAA,CAAQxF,MAAA,CAAO2F,KAAA,EAAO;QAC3C,IAAI;UACF,MAAMC,YAAA,GAAe1B,SAAA;UACrB,MAAM2B,gBAAA,GAAmB,MAAM,KAAKC,cAAA,CAAeN,QAAA,CAAQ1G,IAAI;UAC/D0G,QAAA,CAAQxF,MAAA,CAAO2F,KAAA,GAAQ;YACrBI,aAAA,EAAeH,YAAA;YACfI,iBAAA,EAAmBH,gBAAA;YACnBI,YAAA,EAAcL,YAAA,GAAeC,gBAAA;YAC7BK,SAAA,EAAW;UACb;QACF,SAASnH,GAAA,EAAP,CAGF;MACF;MAEA,OAAOuF,OAAA,CAAQ6B,GAAA,CAAI,CACjB,KAAKrD,cAAA,CAAeiB,cAAc,GAClC,KAAKjB,cAAA,CAAe0C,QAAO,EAC5B,EAAEE,IAAA,CAAK,MAAMF,QAAO;IACvB,CAAC;IAED,IAAIlC,SAAA,EAAW;MACb,IAAII,eAAA,EAAiB;QAGnB;QAAEW,SAAA,CAAkB+B,MAAA,GAAS,MAAM;UACjC1C,eAAA,CAAgB2C,KAAA,EAAM;QACxB;MACF;MAEA,OAAO3J,QAAA,CAAS2H,SAAA,EAAW;QACzBiC,YAAA,EAAchD,SAAA;QACdrD,OAAA,EAAS;MACX,CAAC;IACH,OAAO;MACL,OAAOoE,SAAA;IACT;EACF;EAEA,IAAIrD,OAAA,EAAiB;IACnB,OAAO,KAAKW,OAAA;EACd;EAEA,IAAIX,OAAOA,MAAA,EAAgB;IACzB,KAAKW,OAAA,GAAUX,MAAA;EACjB;EAEA,IAAIC,OAAA,EAAiB;IACnB,OAAO,KAAKW,OAAA;EACd;EAEA,IAAIX,OAAOA,MAAA,EAAgB;IACzB,KAAKW,OAAA,GAAUX,MAAA;EACjB;EAEA,MAAgBkD,eAAerF,IAAA,EAAciC,IAAA,EAAgC;IAC3E,MAAM;MAAEO,aAAA,GAAgB,KAAKe;IAAe,IAAItB,IAAA;IAChD,IAAI;MAAEqC;IAAgB,IAAIrC,IAAA;IAE1B,MAAMwF,SAAA,GAAY5F,kBAAA;IAClB,MAAM6F,cAAA,GAAiB5F,uBAAA;IAEvB,MAAM6F,YAAA,GAAe,KAAK/D,eAAA,GAAkB,KAAKC,kBAAA;IACjD,IAAIqB,QAAA,GAAwD,EAAC;IAE7D,IAAI1C,aAAA,EAAe;MACjB0C,QAAA,CAAS0C,IAAA,CAAK;QACZ7C,IAAA,EAAM;QACNuB,OAAA,EAAS9D;MACX,CAAC;IACH;IAEA,MAAMqF,mBAAA,GAAsB3C,QAAA,CAAS1F,MAAA;IACrC,IAAIsI,YAAA,GAAe9H,IAAA,GACfkF,QAAA,CAAS6C,MAAA,CAAO,CACd;MACEhD,IAAA,EAAM;MACNuB,OAAA,EAAStG,IAAA;MACTgI,IAAA,EAAM/F,IAAA,CAAK+F;IACb,EACD,IACD9C,QAAA;IACJ,IAAIE,SAAA,GAAY;IAEhB,GAAG;MACD,MAAM6C,MAAA,GAASH,YAAA,CACZI,MAAA,CAAO,CAACC,OAAA,EAAQhH,OAAA,KAAY;QAC3B,QAAQA,OAAA,CAAQ4D,IAAA;UACd,KAAK;YACH,OAAOoD,OAAA,CAAOJ,MAAA,CAAO,CAAC;AAAA,EAAkB5G,OAAA,CAAQmF,OAAA,EAAS,CAAC;UAC5D,KAAK;YACH,OAAO6B,OAAA,CAAOJ,MAAA,CAAO,CAAC,GAAGN,SAAA;AAAA,EAAetG,OAAA,CAAQmF,OAAA,EAAS,CAAC;UAC5D;YACE,OAAO6B,OAAA,CAAOJ,MAAA,CAAO,CAAC,GAAGL,cAAA;AAAA,EAAoBvG,OAAA,CAAQmF,OAAA,EAAS,CAAC;QAAA;MAErE,GAAG,EAAc,EAChB8B,IAAA,CAAK,MAAM;MAEd,MAAMC,qBAAA,GAAwB,MAAM,KAAKrB,cAAA,CAAeiB,MAAM;MAC9D,MAAMK,aAAA,GAAgBD,qBAAA,IAAyBV,YAAA;MAE/C,IAAIM,MAAA,IAAU,CAACK,aAAA,EAAe;QAC5B;MACF;MAEApD,QAAA,GAAW4C,YAAA;MACX1C,SAAA,GAAYiD,qBAAA;MAEZ,IAAI,CAACC,aAAA,EAAe;QAClB;MACF;MAEA,IAAI,CAAChE,eAAA,EAAiB;QACpB;MACF;MAEA,MAAMiE,aAAA,GAAgB,MAAM,KAAKzE,eAAA,CAAgBQ,eAAe;MAChE,IAAI,CAACiE,aAAA,EAAe;QAClB;MACF;MAEA,MAAMC,iBAAA,GAAoBD,aAAA,CAAcxD,IAAA,IAAQ;MAEhD+C,YAAA,GAAeA,YAAA,CAAaW,KAAA,CAAM,GAAGZ,mBAAmB,EAAEE,MAAA,CAAO,CAC/D;QACEhD,IAAA,EAAMyD,iBAAA;QACNlC,OAAA,EAASiC,aAAA,CAAcvI,IAAA;QACvBgI,IAAA,EAAMO,aAAA,CAAcP;MACtB,GACA,GAAGF,YAAA,CAAaW,KAAA,CAAMZ,mBAAmB,EAC1C;MAEDvD,eAAA,GAAkBiE,aAAA,CAAcjE,eAAA;IAClC,SAAS;IAIT,MAAMa,SAAA,GAAYuD,IAAA,CAAKC,GAAA,CACrB,GACAD,IAAA,CAAKE,GAAA,CAAI,KAAKhF,eAAA,GAAkBwB,SAAA,EAAW,KAAKvB,kBAAkB,EACpE;IAEA,OAAO;MAAEqB,QAAA;MAAUC,SAAA;MAAWC;IAAU;EAC1C;EAEA,MAAgB4B,eAAehH,IAAA,EAAc;IAE3CA,IAAA,GAAOA,IAAA,CAAK6I,OAAA,CAAQ,oBAAoB,EAAE;IAE1C,OAAiB3K,MAAA,CAAO8B,IAAI,EAAER,MAAA;EAChC;EAEA,MAAgBuE,uBACdiB,EAAA,EAC4B;IAC5B,MAAMnF,GAAA,GAAM,MAAM,KAAKqE,aAAA,CAAc4E,GAAA,CAAI9D,EAAE;IAC3C,OAAOnF,GAAA;EACT;EAEA,MAAgBoE,sBACd9C,OAAA,EACe;IACf,MAAM,KAAK+C,aAAA,CAAc6E,GAAA,CAAI5H,OAAA,CAAQ6D,EAAA,EAAI7D,OAAO;EAClD;AACF;;;AMrdA,OAAO6H,SAAA,MAAc;AACrB,SAASlL,EAAA,IAAMmL,OAAA,QAAc;;;ACD7B,IAAMC,QAAA,GACJ;AAEK,SAASC,cAAc1H,GAAA,EAAsB;EAClD,OAAOA,GAAA,IAAOyH,QAAA,CAASE,IAAA,CAAK3H,GAAG;AACjC;;;ADGO,IAAM4H,yBAAA,GAAN,MAAgC;EAAA;AAAA;AAAA;EAWrCrH,YAAYC,IAAA,EAgBT;IACD,MAAM;MACJqH,WAAA;MACAC,kBAAA,GAAqB;MACrBpG,KAAA,GAAQ;MACRd,KAAA,GAAQ;MACRuD,OAAA;MACApH,KAAA,EAAAc,MAAA,GAAQd;IACV,IAAIyD,IAAA;IAEJ,KAAKuH,YAAA,GAAeF,WAAA;IACpB,KAAKG,mBAAA,GAAsBF,kBAAA;IAC3B,KAAKvG,MAAA,GAAS,CAAC,CAACX,KAAA;IAChB,KAAKqH,MAAA,GAASvG,KAAA;IACd,KAAKF,MAAA,GAAS3D,MAAA;IACd,KAAKqK,QAAA,GAAW/D,OAAA;IAEhB,IAAI,CAAC,KAAK4D,YAAA,EAAc;MACtB,MAAM,IAAInL,KAAA,CAAM,6BAA6B;IAC/C;IAEA,IAAI,CAAC,KAAK4E,MAAA,EAAQ;MAChB,MAAM,IAAI5E,KAAA,CAAM,2CAA2C;IAC7D;IAEA,IAAI,OAAO,KAAK4E,MAAA,KAAW,YAAY;MACrC,MAAM,IAAI5E,KAAA,CAAM,mCAAmC;IACrD;EACF;EAEA,IAAIiL,YAAA,EAAsB;IACxB,OAAO,KAAKE,YAAA;EACd;EAEA,IAAIF,YAAYtK,KAAA,EAAe;IAC7B,KAAKwK,YAAA,GAAexK,KAAA;EACtB;EAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;EAyBA,MAAMqF,YACJrE,IAAA,EAE4B;IAAA,IAD5BiC,IAAA,GAAA1C,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAwC,CAAC;IAEzC,IAAI,CAAC,CAAC0C,IAAA,CAAKyC,cAAA,KAAmB,CAAC,CAACzC,IAAA,CAAKqC,eAAA,EAAiB;MACpD,MAAM,IAAIjG,KAAA,CACR,kHACF;IACF;IAEA,IAAI4D,IAAA,CAAKyC,cAAA,IAAkB,CAACyE,aAAA,CAAclH,IAAA,CAAKyC,cAAc,GAAG;MAC9D,MAAM,IAAIrG,KAAA,CACR,+EACF;IACF;IAEA,IAAI4D,IAAA,CAAKqC,eAAA,IAAmB,CAAC6E,aAAA,CAAclH,IAAA,CAAKqC,eAAe,GAAG;MAChE,MAAM,IAAIjG,KAAA,CACR,gFACF;IACF;IAEA,IAAI4D,IAAA,CAAKsC,SAAA,IAAa,CAAC4E,aAAA,CAAclH,IAAA,CAAKsC,SAAS,GAAG;MACpD,MAAM,IAAIlG,KAAA,CACR,0EACF;IACF;IAEA,MAAM;MACJqG,cAAA;MACAJ,eAAA,GAAkB2E,OAAA,EAAO;MACzB1E,SAAA,GAAY0E,OAAA,EAAO;MACnBW,MAAA,GAAS;MACTpF,SAAA;MACAC;IACF,IAAIxC,IAAA;IAEJ,IAAI;MAAE0C;IAAY,IAAI1C,IAAA;IAEtB,IAAI2C,eAAA,GAAmC;IACvC,IAAIJ,SAAA,IAAa,CAACG,WAAA,EAAa;MAC7BC,eAAA,GAAkB,IAAIC,eAAA,EAAgB;MACtCF,WAAA,GAAcC,eAAA,CAAgBE,MAAA;IAChC;IAEA,MAAMxD,IAAA,GAAmC;MACvCsI,MAAA;MACA1E,QAAA,EAAU,CACR;QACEF,EAAA,EAAIT,SAAA;QACJQ,IAAA,EAAM;QACNuB,OAAA,EAAS;UACPuD,YAAA,EAAc;UACdC,KAAA,EAAO,CAAC9J,IAAI;QACd;MACF,EACF;MACAmD,KAAA,EAAO,KAAKuG,MAAA;MACZK,iBAAA,EAAmBzF;IACrB;IAEA,IAAII,cAAA,EAAgB;MAClBpD,IAAA,CAAK0I,eAAA,GAAkBtF,cAAA;IACzB;IAEA,MAAMY,MAAA,GAA4B;MAChCP,IAAA,EAAM;MACNC,EAAA,EAAIiE,OAAA,EAAO;MACX3E,eAAA,EAAiBC,SAAA;MACjBG,cAAA;MACA1E,IAAA,EAAM;IACR;IAEA,MAAMuF,SAAA,GAAY,IAAIC,OAAA,CAA2B,CAACC,OAAA,EAASC,MAAA,KAAW;MACpE,MAAMtG,GAAA,GAAM,KAAKqK,mBAAA;MACjB,MAAM7D,OAAA,GAAU;QACd,GAAG,KAAK+D,QAAA;QACR9D,aAAA,EAAe,UAAU,KAAK2D,YAAA;QAC9BS,MAAA,EAAQ;QACR,gBAAgB;MAClB;MAEA,IAAI,KAAKjH,MAAA,EAAQ;QACf3B,OAAA,CAAQ0E,GAAA,CAAI,QAAQ3G,GAAA,EAAK;UAAEkC,IAAA;UAAMsE;QAAQ,CAAC;MAC5C;MAEAzG,QAAA,CACEC,GAAA,EACA;QACE4G,MAAA,EAAQ;QACRJ,OAAA;QACAtE,IAAA,EAAMN,IAAA,CAAKiF,SAAA,CAAU3E,IAAI;QACzBwD,MAAA,EAAQH,WAAA;QACRjF,SAAA,EAAYiB,IAAA,IAAiB;UA7LvC,IAAAG,EAAA,EAAA6E,EAAA,EAAAuE,EAAA;UA8LY,IAAIvJ,IAAA,KAAS,UAAU;YACrB,OAAO8E,OAAA,CAAQH,MAAM;UACvB;UAEA,IAAI;YACF,MAAM6E,kBAAA,GACJnJ,IAAA,CAAKC,KAAA,CAAMN,IAAI;YACjB,IAAIwJ,kBAAA,CAAmBH,eAAA,EAAiB;cACtC1E,MAAA,CAAOZ,cAAA,GAAiByF,kBAAA,CAAmBH,eAAA;YAC7C;YAEA,KAAIlJ,EAAA,GAAAqJ,kBAAA,CAAmBhJ,OAAA,KAAnB,gBAAAL,EAAA,CAA4BkE,EAAA,EAAI;cAClCM,MAAA,CAAON,EAAA,GAAKmF,kBAAA,CAAmBhJ,OAAA,CAAQ6D,EAAA;YACzC;YAEA,MAAM7D,OAAA,GAAUgJ,kBAAA,CAAmBhJ,OAAA;YAGnC,IAAIA,OAAA,EAAS;cACX,IAAIiJ,KAAA,IAAOF,EAAA,IAAAvE,EAAA,GAAAxE,OAAA,oBAAAA,OAAA,CAASmF,OAAA,KAAT,gBAAAX,EAAA,CAAkBmE,KAAA,KAAlB,gBAAAI,EAAA,CAA0B;cAErC,IAAIE,KAAA,EAAM;gBACR9E,MAAA,CAAOtF,IAAA,GAAOoK,KAAA;gBAEd,IAAI3F,UAAA,EAAY;kBACdA,UAAA,CAAWa,MAAM;gBACnB;cACF;YACF;UACF,SAASrF,GAAA,EAAP;YACAyF,MAAA,CAAOzF,GAAG;UACZ;QACF;QACAN,OAAA,EAAUM,GAAA,IAAQ;UAChByF,MAAA,CAAOzF,GAAG;QACZ;MACF,GACA,KAAKgD,MAAA,CACP,CAAEuD,KAAA,CAAOvG,GAAA,IAAQ;QACf,MAAMoK,WAAA,GAAcpK,GAAA,CAAIuB,QAAA,EAAS,CAAE8I,WAAA,EAAY;QAE/C,IACEhF,MAAA,CAAOtF,IAAA,KACNqK,WAAA,KAAgB,kCACfA,WAAA,KAAgB,0BAClB;UAKA,OAAO5E,OAAA,CAAQH,MAAM;QACvB,OAAO;UACL,OAAOI,MAAA,CAAOzF,GAAG;QACnB;MACF,CAAC;IACH,CAAC;IAED,IAAIuE,SAAA,EAAW;MACb,IAAII,eAAA,EAAiB;QAGnB;QAAEW,SAAA,CAAkB+B,MAAA,GAAS,MAAM;UACjC1C,eAAA,CAAgB2C,KAAA,EAAM;QACxB;MACF;MAEA,OAAOyB,SAAA,CAASzD,SAAA,EAAW;QACzBiC,YAAA,EAAchD,SAAA;QACdrD,OAAA,EAAS;MACX,CAAC;IACH,OAAO;MACL,OAAOoE,SAAA;IACT;EACF;AACF"},"metadata":{},"sourceType":"module","externalDependencies":[]}